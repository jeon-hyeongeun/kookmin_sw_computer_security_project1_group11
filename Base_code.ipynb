{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "c823e32c24d06b3518059b5f5e11ccae729689482027ea5186dc849db7650ef7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit ('torch': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Base_code.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9b1n4HbzgED"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ahq1xVvzgEF"
      },
      "source": [
        "파싱"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxGTJeLazgEH"
      },
      "source": [
        "\n",
        "def parsing(path):#파싱을 진행하는 함수\n",
        "    with open(path,'r',encoding='utf-8') as f:#파일을 읽어드리고 ['로그','로그',...] 이런식으로 로그를 구조화\n",
        "        train=[]\n",
        "        para=\"\"\n",
        "        while True:\n",
        "            l = f.readline() #한줄씩 읽어 옵니다\n",
        "\n",
        "            if not l:\n",
        "                break #파일을 전부 읽으면 읽기를 중단합니다.\n",
        "\n",
        "            if l != \"\\n\":\n",
        "                para +=l\n",
        "            else:\n",
        "                if para!='':\n",
        "                    if para[:4]=='POST': #Method가 POST인 경우 예외적으로 바디까지 가져옵니다.\n",
        "                        para+=f.readline()\n",
        "                    train.append(para)\n",
        "                    para=\"\"\n",
        "    return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuk5EPLzgEI"
      },
      "source": [
        "def dataset(path,mod='train'): #데이터셋을 생성합니다. 파싱한 데이터와 라벨을 생성합니다 \n",
        "    x = parsing(f'{path}norm_{mod}.txt') # mod에 따라 train을 가져올지 test 데이터를 가져올지 결정됩니다.\n",
        "    y = [0]*len(x) # 정상 라벨 0 을 정상 데이터 개수 만큼 생성\n",
        "    x += parsing(f'{path}anomal_{mod}.txt')\n",
        "    y += [1]*(len(x)-len(y)) # 비정상 라벨 1을 비정상 데이터 개수 만큼 생성\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y33EpYD5zgEJ"
      },
      "source": [
        "def vectorize(train_x,test_x): #문장을 벡터로 만듭니다 해당 코드에서는 기본적인 tf idf를 사용하고 있습니다.\n",
        "    tf = TfidfVectorizer()\n",
        "    tf = tf.fit(train_x)\n",
        "    train_vec = tf.transform(train_x)\n",
        "    test_vec = tf.transform(test_x)\n",
        "    return train_vec,test_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS2BONUEzgEK"
      },
      "source": [
        "def train(train_vec,train_y): #랜덤 포레스트로 훈련 시킵니다. 모델을 바꾸고 싶다면 이 함수를 변경해야 합니다.\n",
        "    rf = RandomForestClassifier()\n",
        "    rf.fit(train_vec,train_y)\n",
        "    return rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwVBlk72zgEK"
      },
      "source": [
        "def test(test_y,test_vec,rf): #입렵 받은 테스트와 모델로 테스트를 실시합니다\n",
        "    pred = rf.predict(test_vec)\n",
        "    print(accuracy_score(test_y,pred))\n",
        "    print(f1_score(test_y,pred))\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1AXJmDhzgEL"
      },
      "source": [
        "############### 실행 코드 #######################\n",
        "################################################\n",
        "train_x, train_y = dataset('./','train')\n",
        "test_x, test_y =  dataset('./','test')\n",
        "\n",
        "train_vec, test_vec = vectorize(train_x, test_x)\n",
        "rf = train(train_vec, train_y)\n",
        "pred = test(test_y,test_vec, rf)\n",
        "\n",
        "########################################################\n",
        "tf = TfidfVectorizer()\n",
        "tf = tf.fit(train_x)\n",
        "\n",
        "# print(len(tf.vocabulary_)) # 고유한 단어가 대략 8만개가 나옵니다\n",
        "\n",
        "# print(tf.transform(train_x)[0]) #로그 하나당 약 8만차원이 나옵니다\n",
        "                            # 필요없는 문장 때문에 단어의 개수가 많이 진것일  수도 있으니 데이터를 분석하여 필요한 부분만 사용하는것이 좋습니다\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}